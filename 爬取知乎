import time
import re
import os
import requests
from bs4 import BeautifulSoup
import chardet

proxy = ''
proxies = {
    'http': 'http://' + proxy,
    'https': 'https://' + proxy,
}

def get_list():
    url = 'https://www.zhihu.com/api/v4/columns/%s/articles?include=data[*].topics&limit=1' % author
    article_dict = {}
    while True:
        print('fetching', url)
        data = None
        try:
            print(url)
            resp = requests.get(url, headers=headers, proxies=proxies)
            j = resp.json()
            data = j['data']
            
        except Exception as e:
            print(e, 'get list failed')

        for article in data:
            aid = article['id']
            akeys = article_dict.keys()
            if aid not in akeys:
                article_dict[aid] = article['title']
        # todo(zy)
        break

        if j['paging']['is_end']:
            break
        url = j['paging']['next']
        time.sleep(2)

    with open('zhihu_ids.txt', 'w', encoding='utf-8') as f:
        items = sorted(article_dict.items())
        for item in items:
            # print(item)
            print(chardet.detect(item[1].encode()))
            # input()
            f.write('%s %s\n' % item)

def get_html(aid, title, index):
    title = title.replace('/', '／')
    title = title.replace('\\', '＼')
    file_name = '%03d.%s.html' % (index, title)
    if os.path.exists(file_name):
        print(title, 'already exists.')
        return
    else:
        print('saving', title)
    try:
        url = 'https://zhuanlan.zhihu.com/p/' + aid
        html = requests.get(url,  headers=headers, proxies=proxies).text
        soup = BeautifulSoup(html, 'lxml')
        content = soup.find(class_='Post-RichText').prettify()
        content = content.replace('data-actual', '')
        content = content.replace('h1>', 'h2>')
        content = re.sub(r'<noscript>.*?</noscript>', '', content)
        content = re.sub(r'src="data:image.*?"', '', content)
        content = '<!DOCTYPE html><html><head><meta charset="utf-8"></head><body><h1>%s</h1>%s</body></html>' % (
            title, content)
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(content)
    except Exception as e:
        print(e, 'get %s failed' % title)
    time.sleep(2)

def get_details():
    with open('zhihu_ids.txt', encoding='utf-8') as f:
        i = 1
        for line in f:
            lst = line.strip().split(' ')
            aid = lst[0]
            # todo(zy)
            # import pdb;pdb.set_trace()
            title = '_'.join(lst[1:])
            get_html(aid, title, i)
            i += 1

def to_pdf():
    import pdfkit
    print('exporting PDF...')
    for root, dirs, files in os.walk('.'):
        htmls = [name for name in files if name.endswith(".html")]
    path_wk = r'C:\Program Files\wkhtmltopdf\bin\wkhtmltopdf.exe'
    config = pdfkit.configuration(wkhtmltopdf=path_wk)
    options = {
        'page-size': 'Letter',
        'encoding': 'UTF-8',
        'custom-header': [('Accept-Encoding', 'gzip')],
        '--proxy': 'http://' + proxy
    }
    print(htmls)
    pdfkit.from_file(htmls[0], author + '.pdf', configuration=config, options=options)

if __name__ == '__main__':
    author = input('Please input author name:(default crossin)')
    if not author:
        author = 'crossin'
    headers = {
        'origin': 'https://zhuanlan.zhihu.com',
        'referer': 'https://zhuanlan.zhihu.com/%s' % author,
        'User-Agent': ('Mozilla/5.0'),
    }
    # get_list()
    # get_details()
    to_pdf()
